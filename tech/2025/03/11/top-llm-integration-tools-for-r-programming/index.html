<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" itemscope itemtype="https://schema.org/WebApplication" lang="en"
  prefix="og: https://ogp.me/ns#">

<head>
  <meta charset="utf-8">
  <title>3 of the best LLM integration tools for R - Major Digest</title>
  <meta name="description" content="When we first looked at this space in late 2023, many generative AI R packages focused largely on OpenAI and ChatGPT-like functionality or coding add-ins. Today, the landscape includes packages that natively support more LLM suppliers—including models run locally on your own computer.">
  <link rel="canonical" href="https://majordigest.com/tech/2025/03/11/top-llm-integration-tools-for-r-programming/">

  <meta itemprop="name" content="3 of the best LLM integration tools for R - Major Digest">
  <meta itemprop="description" content="When we first looked at this space in late 2023, many generative AI R packages focused largely on OpenAI and ChatGPT-like functionality or coding add-ins. Today, the landscape includes packages that natively support more LLM suppliers—including models run locally on your own computer.">
  <link itemprop="url" href="https://majordigest.com/tech/2025/03/11/top-llm-integration-tools-for-r-programming/">
  <meta itemprop="image" content="https://majordigest.com/static12/tech/2025/03/11/top-llm-integration-tools-for-r-programming.webp">

  <meta property="og:title" content="3 of the best LLM integration tools for R - Major Digest">
  <meta property="og:description" content="When we first looked at this space in late 2023, many generative AI R packages focused largely on OpenAI and ChatGPT-like functionality or coding add-ins. Today, the landscape includes packages that natively support more LLM suppliers—including models run locally on your own computer.">
  <meta property="og:url" content="https://majordigest.com/tech/2025/03/11/top-llm-integration-tools-for-r-programming/">
  <meta property="og:image" content="https://majordigest.com/static12/tech/2025/03/11/top-llm-integration-tools-for-r-programming.webp">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Major Digest">
  <meta property="og:locale" content="en_US">
  <meta property="fb:pages" content="113570554924596">
  <!-- <meta property="fb:app_id" content="490025408049997"> -->

  <meta name="twitter:title" content="3 of the best LLM integration tools for R - Major Digest">
  <meta name="twitter:description" content="When we first looked at this space in late 2023, many generative AI R packages focused largely on OpenAI and ChatGPT-like functionality or coding add-ins. Today, the landscape includes packages that natively support more LLM suppliers—including models run locally on your own computer.">
  <meta name="twitter:url" content="https://majordigest.com/tech/2025/03/11/top-llm-integration-tools-for-r-programming/">
  <meta name="twitter:image" content="https://majordigest.com/static12/tech/2025/03/11/top-llm-integration-tools-for-r-programming.webp">
  <meta name="twitter:image:alt" content="3 of the best LLM integration tools for R - Major Digest">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@major_digest">
  <meta name="twitter:creator" content="@vpodk">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=5.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
  <link rel="apple-touch-startup-image" href="/assets/icons/logo-512x512.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="Major Digest">
  <meta name="application-name" content="Major Digest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel="manifest" href="/manifest.json?v=1.1.6">
  <link rel="stylesheet" href="/assets/styles.css?v=1.1.6">
  <link rel="alternate" type="application/rss+xml" title="Major Digest RSS Feed"
    href="https://majordigest.com/tech/feed.xml">
  <link rel="sitemap" type="application/xml" title="Major Digest Sitemap"
    href="https://majordigest.com/tech/sitemap.xml">
</head>

<body class="tech">
  <a href="#main" class="skip-nav">Skip to Main Content</a>
  <header>
    <span>
      <time datetime="2025-03-11T12:00:28.006Z" itemprop="datePublished">Tuesday, March 11, 2025</time> &nbsp;
    </span>
    <h1>
      <a href="/" aria-label="Major Digest Home"><img src="/assets/logo.svg" alt="Major Digest Home" width="225"
          height="50"></a>
      <span>3 of the best LLM integration tools for R - Major Digest</span>
    </h1>
  </header>
  <nav itemscope itemtype="https://schema.org/SiteNavigationElement">
    <a itemprop="url" href="/us/" title="The Latest U.S. News From Most Reliable Sources">U.S.</a>
    <a itemprop="url" href="/world/" title="Breaking News From Around the World">World</a>
    <a itemprop="url" href="/tech/" title="The Latest Tech News and Headlines">Technology</a>
    <a itemprop="url" href="/sports/" title="Stay Up to Date on Your Favorite Teams and Players">Sports</a>
    <a itemprop="url" href="/politics/" title="The Latest Political News and Headlines">Politics</a>
  </nav>
  <main id="main" aria-label="Main content">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
    "@type": "ListItem", "position": 1, "name": "Home",
    "item": "https://majordigest.com/"
  }, {
    "@type": "ListItem", "position": 2, "name": "tech",
    "item": "https://majordigest.com/tech/"
  }, {
    "@type": "ListItem", "position": 3, "name": "3 of the best LLM integration tools for R - Major Digest"
  }]
}
</script>
<div itemscope itemtype="https://schema.org/NewsArticle" class="article">
  <meta itemprop="dateModified" content="Tue, 11 Mar 2025 09:00:00 GMT">
  <meta itemprop="url" content="/tech/2025/03/11/top-llm-integration-tools-for-r-programming/">
  <h2 itemprop="headline">3 of the best LLM integration tools for R</h2>
  <div itemprop="articleBody">
    <figure>
      <img src="/static12/tech/2025/03/11/top-llm-integration-tools-for-r-programming.webp" alt="3 of the best LLM integration tools for R" itemprop="image" width="400" height="225"
        data-src="https://www.infoworld.com/wp-content/uploads/2025/03/3828268-0-47146300-1741683725-shutterstock_1639067131-100963253-orig.jpg?quality=50&strip=all">
      <figcaption>Credit: Info World</figcaption>
    </figure>
    <div id="remove_no_follow">
		<div class="grid grid--cols-10@md grid--cols-8@lg article-column">
					  <div class="col-12 col-10@md col-6@lg col-start-3@lg">
						<div class="article-column__content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section>



<p>When we first looked at this space in late 2023, many generative AI R packages focused largely on OpenAI and ChatGPT-like functionality or coding add-ins. Today, the landscape includes packages that natively support more LLM suppliers—including models run locally on your own computer. The range of genAI tasks you can do in R has also broadened.</p>



<p>Here’s an updated look at one of the more intriguing categories in the landscape of generative AI packages for R: adding large language model capabilities to your own scripts and apps. My next article will cover tools for getting help with your R programming and running LLMs locally.</p>



<h3 class="wp-block-heading" id="ellmer">ellmer</h3>



<p>ellmer wasn’t one of the earliest entries in this space, but as the main Tidyverse package for using LLMs in an R workflow, it’s among the most important. ellmer has the resources of Posit (formerly RStudio) behind it, and its co-authors—Posit Chief Scientist Hadley Wickham, (creator of ggplot2 and <code>dplyr</code>) and CTO Joe Cheng, original author of the Shiny R web framework—are well known for their work on other popular R packages.</p>



<p>Kyle Walker, author of several R and Python packages including <code>tidycensus</code>, posted on Bluesky: “{ellmer} is the best #LLM interface in any language, in my opinion.” (Walker recently contributed code to ellmer for processing PDF files.)</p>



<h3 class="wp-block-heading" id="getting-started-with-ellmer">Getting started with ellmer</h3>



<p>ellmer supports a range of features, use cases, and platforms, and it’s also well-documented. You can install the <code>ellmer</code> package from CRAN the usual way or try the development version with <code>pak::pak("tidyverse/ellmer")</code>.</p>



<p>To use ellmer, you start by creating a chat object with functions such as <code>chat_openai()</code> or <code>chat_claude()</code>. Then you’ll use that object to interact with the LLM.</p>



<p>Check the chat function help file to see how to store your API key (unless you’re using a local model). For example, <code>chat_openai()</code> expects an environment variable, <code>OPENAI_API_KEY</code>, which you can store in your <code>.Renviron</code> file. A call like <code>chat_anthropic()</code> will look for <code>ANTHROPIC_API_KEY</code>.</p>



<p>ellmer supports around a dozen LLM platforms including OpenAI and Azure OpenAI, Anthropic, Google Gemini, AWS Bedrock, Snowflake Cortex, Perplexity, and Ollama for local models. Here’s an example of creating a basic OpenAI chat object:</p>



<pre class="prettyprint">
<code>
library(ellmer)
my_chat 
</code></pre>




<p>And here’s the syntax for using Claude:</p>



<pre class="prettyprint">
<code>
my_chat_claude 
</code></pre>




<p>System prompts aren’t required, but I like to include them if I’ve got a topic-specific task. Not all chat functions require setting the model, either, but it’s a good idea to specify one so you’re not unpleasantly surprised by either an old, outdated model or a cutting-edge pricey one.</p>



<p>Several R experts said in early 2025 that Anthropic’s Claude Sonnet 3.5 seemed to be best specifically for writing R code, and early reports say Sonnet 3.7 is even better in general at code. However, with the new batch of models released by OpenAI, Google, and others, it’s worth keeping an eye on how that unfolds. This is a good reason for coding in a way that it’s relatively easy to swap models and providers. Separating the chat object from other code should do the trick.</p>



<p>I’ve found OpenAI’s new o3-mini model to be quite good at writing R code. It’s less expensive than Sonnet 3.7, but as a “reasoning” model that always thinks step by step, it may not respond as quickly as conventional models do.</p>



<p>Sonnet is somewhat pricier than GPT-4o, but unless you’re feeding it massive amounts of code, the bill should still be pretty small (my queries typically run two cents each).</p>



<p>If you want to set other parameters for the model, including temperature (meaning the amount of randomness acceptable in the response), use <code>api_arg</code>. That takes a named list like this one:</p>



<pre class="prettyprint">
<code>
my_chat 
</code></pre>




<p>A higher temperature is often viewed as more creative, whereas a lower one is considered more precise. Also note that <em>temperature</em> isn’t an explicit argument in ellmer’s chat functions, so you need to remember to set it in the <code>api_args</code> named list. (Updating this setting is a good idea since temperature can be an important parameter when working with LLMs.)</p>



<p>Check the model provider’s API documentation for a list of available argument names and valid value ranges for temperature and other settings.</p>



<h3 class="wp-block-heading" id="interactivity">Interactivity</h3>



<p>ellmer offers three levels of interactivity for using its chat objects: a chatbot interface, either in the R console or browser only; streaming and saving the results as text; or only storing query results without streaming.</p>



<p><code>live_console(my_chat)</code> opens <code>my_chat</code> in a chat interface in the R console. <code>live_browser(my_chat)</code> creates a basic chatbot interface in a browser. In both cases, responses are streamed.</p>



<p>ellmer’s “interactive method call” uses a syntax like <code>my_chat$chat()</code> to both display streamed responses and save the result as text:</p>



<pre class="prettyprint">
<code>
my_results 
</code></pre>




<p>If you use <code>my_chat</code> again to ask another question, you’ll find the chat retained your previous question-and-answer history, so you can refer to your previous questions and the responses. For example, the following will refer back to the initial question:</p>



<pre class="prettyprint">
<code>
my_results2 <- my_chat$chat("Now color turquoise please")
</code>
</pre>




<p></p>



<p>This chat query includes a history of previous questions and responses.</p>



<p>When using ellmer code non-interactively within an R script as part of a larger workflow, the documentation suggests wrapping your code inside an R function, like so:</p>



<pre class="prettyprint">
<code>
my_function 
</code></pre>




<p>In this case, <code>my_answer</code> is a simple text string. </p>



<p>The package also includes documentation for tool/function calling, prompt design, extracting structured data, and more. I’ve used it along with the shinychat package to add chatbot capabilities to a Shiny app. You can see example code that creates a simple Shiny chat interface for Ollama local models on your system, including dropdown model selection and a button to download the chat, in this public GitHub gist.</p>



<p>Note that ellmer started life as <em>elmer</em>, but they are branches of the same resulting package.</p>



<h3 class="wp-block-heading" id="tidyllm">tidyllm</h3>



<p>Some of tidyllm’s capabilities overlap with ellmer’s, but its makers say the interface and design philosophy are very different. For one thing, tidyllm combines “verbs”—the type of request you want to make, such as <code>chat()</code> or <code>send_batch()</code>—with providers. Requests include text embeddings (generating numerical vectors to quantify the semantic meaning of a text string) as well as chats and batch processing.</p>



<p>tidyllm currently supports Anthropic Claude models, OpenAI, Google Gemini, Mistral, Groq (which is not grok), Perplexity, Azure, and local Ollama models. If using a cloud provider, you’ll need to set up your API key either with <code>Sys.setenv()</code> or in your <code>.Renviron</code> file.</p>



<h3 class="wp-block-heading" id="querying">Querying</h3>



<p>A simple chat query typically starts by creating an <code>LLMMessage</code> object via the <code>llm_message("your query here")</code> function, and then piping that to a <code>chat()</code> function, such as</p>



<pre class="prettyprint">
<code>
library(tidyllm)

my_conversation 
  chat(openai(.model = "gpt-4o-mini", .temperature = 0, .stream = FALSE))
</code>
</pre>




<p>The <code>llm_message()</code> can also include a <code>.system_prompt</code> message.</p>



<p>The returned value of <code>my_conversation</code> is just a text string with the model’s response, which is easy to print out.</p>



<p>You can build on that by adding another message and chat, which will keep the original query and response in memory, such as:</p>



<pre class="prettyprint">
<code>
# Keep history of last message
my_conversation2 
  llm_message("How would I rotate labels on the x-axis 90 degrees?") |>
  chat(openai(.model = "gpt-4o-mini", .temperature = 0, .stream = FALSE))
print(my_conversation2)
</code>
</pre>




<p>It’s possible to extract some metadata from the results with</p>



<pre class="prettyprint">
<code>
result_metadata 
</code></pre>




<p>which returns a <em>tibble</em> (a special data frame) with columns for <code>model</code>, <code>timestamp</code>, <code>prompt_tokens,</code> <code>completion_tokens</code>, <code>total_tokens</code> (which oddly was the same as <code>completion_tokens</code> when I tested in some cases), and <code>api_specific </code>lists with more token details. You can also extract the user message that generated the reply with <code>get_user_message(my_conversation2)</code>.</p>



<p><code>llm_message()</code> also supports sending images to models that support such uploads with the <code>.imagefile</code> argument. And, you can ask questions about PDF files with the <code>.pdf</code> argument (make sure you also have the <code>pdftools</code> R package installed). For example, I downloaded the tidyllm PDF reference manual from CRAN to a file named <code>tidyllm.pdf</code> in a files subdirectory, and then asked a question using tidyllm with R:</p>



<pre class="prettyprint">
<code>
my_conversation3 
  chat(openai(.model = "gpt-4o", .temperature = .1, .stream = FALSE))
</code>
</pre>




<aside class="sidebar large">
<h3>print(my_conversation3)<h3 id="">
</h3></h3><p>Message History: system: You are a helpful assistant ————————————————————– user: What are 10 interesting tasks I can perform with the tidyllm package? -> Attached Media Files: tidyllm.pdf ————————————————————– assistant: The tidyllm package provides a tidy interface for integrating large language model (LLM) APIs into R workflows. Here are 10 interesting tasks you can perform with the tidyllm package:</p>


<ol>
<li><strong>Chat with Language Models</strong>: Use the chat() function to send messages to various language models like OpenAI, Claude, or Mistral, and receive responses. This can be useful for generating text, answering questions, or creating conversational agents.</li>

<li><strong>Generate Text Embeddings</strong>: Use the embed() function to generate embeddings for text data. This can be useful for tasks like semantic search, clustering, or as input features for machine learning models.</li>

<li><strong>Batch Processing</strong>: Use functions like send_batch(), fetch_batch(), and check_batch() to send multiple requests in a batch to an LLM API, retrieve results, and check the status of batch processing. This is useful for processing large datasets efficiently.</li>

<li><strong>Multimodal Interactions</strong>: Create LLMMessage objects that include text and media (such as images or PDFs) for multimodal processing. This can be useful for tasks that require understanding both text and visual content.</li>

<li><strong>Interactive Message History</strong>: Maintain a conversation history using LLMMessage objects, allowing for context-aware interactions with language models. This is useful for building chatbots or virtual assistants that need to remember previous interactions.</li>

<li><strong>Custom JSON Schemas</strong>: Define custom JSON schemas using tidyllm_schema() to enforce structured outputs from language models. This is useful for ensuring that the responses from the models adhere to a specific format.</li>

<li><strong>Transcribe Audio Files</strong>: Use the groq_transcribe() function to transcribe audio files using the Groq transcription API. This can be useful for converting spoken content into text.</li>

<li><strong>File Management with Gemini API</strong>: Use functions like gemini_upload_file(), gemini_list_files(), and gemini_delete_file() to manage files in the Gemini API. This is useful for tasks that involve uploading, listing, or deleting files as part of the workflow.</li>

<li><strong>Rate Limit Information</strong>: Use rate_limit_info() to retrieve the current rate limit information for APIs. This is useful for managing API usage and avoiding hitting rate limits.</li>

<li><strong>Stream Responses</strong>: Enable streaming of responses from language models using the .stream parameter in functions like chat() or ollama_chat(). This is useful for real-time applications where you want to display the response as it is being generated.</li>
</ol>

<p>These tasks demonstrate the versatility of the tidyllm package in integrating LLMs into various data workflows and applications.</p>
</aside>




<p><code>result_metadata3 <- get_metadata(my_conversation3)</code> showed that my query used 22,226 input tokens and 556 output tokens. At $2.50 per million input tokens, that interaction was around 5.5 cents for the query part to include the entire 58-page PDF in the question, plus another half a cent or so for the response (at $10 per million).</p>



<p>I tried the same query with Claude Sonnet 3.5 and I estimate it cost around 7.7 cents for my query, including the entire 58-page PDF, plus another penny or so for the output (which is $15 per million).</p>



<p>However, if I built up several other questions on top of that, the tokens could add up. This code is just to show package syntax and capabilities, by the way; it’s not an efficient way to query large documents. (See the section on RAG and Ragnar later in the article for a more typical approach.)</p>



<p>If you don’t need immediate responses, another way to trim spending is to use lower-cost batch requests. Several providers, including Anthropic, OpenAI, and Mistral, offer discounts of around 50% for batch processing. The tidyllm <code>send_batch()</code> function submits such a request, <code>check_batch() checks on the status, and <code>fetch_batch()</code> gets the results. You can see more details on the tidyllm package website.</code></p>



<p>tidyllm was created by Eduard Brüll, a researcher at the ZEW economic think tank in Germany.</p>



<h3 class="wp-block-heading" id="batchllm">batchLLM</h3>



<p>As the name implies, batchLLM is designed to run prompts over multiple targets. More specifically, you can run a prompt over a column in a data frame and get a data frame in return with a new column of responses. This can be a handy way of incorporating LLMs in an R workflow for tasks such as sentiment analysis, classification, and labeling or tagging.</p>



<p>It also logs batches and metadata, lets you compare results from different LLMs side by side, and has built-in delays for API rate limiting.</p>



<p></p>



<p>batchLLM’s Shiny app offers a handy graphical user interface for running LLM queries and commands on a column of data.</p>



<p>batchLLM also includes a built-in Shiny app that gives you a handy web interface for doing all this work. You can launch the web app with <code>batchLLM_shiny()</code> or as an RStudio add-in, if you use RStudio. There’s also a web demo of the app.</p>



<p>batchLLM’s creator, Dylan Pieper, said he created the package due to the need to categorize “thousands of unique offense descriptions in court data.” However, note that this “batch processing” tool <em>does not use</em> the less expensive, time-delayed LLM calls offered by some model providers. Pieper explained on GitHub that “most of the services didn’t offer it or the API packages didn’t support it” at the time he wrote batchLLM. He also noted that he had preferred real-time responses to asynchronous ones.</p>



<h3 class="wp-block-heading" id="two-more-tools-to-watch">Two more tools to watch</h3>



<p>We’ve looked at three top tools for integrating large language models into R scripts and programs. Now let’s look at a couple more tools that focus on specific tasks when using LLMs within R: retrieving information from large amounts of data, and scripting common prompting tasks.</p>



<h3 class="wp-block-heading" id="ragnar-rag-for-r">ragnar (RAG for R)</h3>



<p>RAG, or retrieval augmented generation, is one of the most useful applications for LLMs. Instead of relying on an LLM’s internal knowledge or directing it to search the web, the LLM generates its response based only on specific information you’ve given it. InfoWorld’s Smart Answers feature is an example of a RAG application, answering tech questions based solely on articles published by InfoWorld and its sister sites.</p>



<p>A RAG process typically involves splitting documents into chunks, using models to generate embeddings for each chunk, embedding a user’s query, and then finding the most relevant text chunks for that query based on calculating which chunks’ embeddings are closest to the query’s. The relevant text chunks are then sent to an LLM along with the original question, and the model answers based on that provided context. This makes it practical to answer questions using many documents as potential sources without having to stuff all the content of those documents into the query.</p>



<p>There are numerous RAG packages and tools for Python and JavaScript, but not many in R beyond generating embeddings. However, the ragnar package, currently very much under development, aims to offer “a complete solution with sensible defaults, while still giving the knowledgeable user precise control over all the steps.”</p>



<p>Those steps either do or will include document processing, chunking, embedding, storage (defaulting to DuckDB), retrieval (based on both embedding similarity search and text search), a technique called re-ranking to improve search results, and prompt generation.</p>



<p>If you’re an R user and interested in RAG, keep an eye on ragnar.</p>



<h3 class="wp-block-heading" id="tidyprompt">tidyprompt</h3>



<p>Serious LLM users will likely want to code certain tasks more than once. Examples include generating structured output, calling functions, or forcing the LLM to respond in a specific way (such as chain-of-thought).</p>



<p>The idea behind the tidyprompt package is to offer “building blocks” to construct prompts and handle LLM output, and then chain those blocks together using conventional R pipes.</p>



<p>tidyprompt “should be seen as a tool which can be used to enhance the functionality of LLMs beyond what APIs natively offer,” according to the package documentation, with functions such as <code>answer_as_json()</code>, <code>answer_as_text()</code>, and <code>answer_using_tools()</code>.</p>



<p>A prompt can be as simple as</p>



<pre class="prettyprint">
<code>
library(tidyprompt)
"Is London the capital of France?" |>
  answer_as_boolean() |>
  send_prompt(llm_provider_groq(parameters = list(model = "llama3-70b-8192") ))
</code>
</pre>




<p>which in this case returns <code>FALSE</code>. (Note that I had first stored my Groq API key in an R environment variable, as would be the case for any cloud LLM provider.) For a more detailed example, check out the Sentiment analysis in R with a LLM and ‘tidyprompt’ vignette on GitHub.</p>



<p>There are also more complex pipelines using functions such as <code>llm_feedback()</code> to check if an LLM response meets certain conditions and <code>user_verify()</code> to make it possible for a human to check an LLM response.</p>



<p>You can create your own <code>tidyprompt</code> prompt wraps with the <code>prompt_wrap()</code> function.</p>



<p>The <code>tidyprompt</code> package supports OpenAI, Google Gemini, Ollama, Groq, Grok, XAI, and OpenRouter (not Anthropic directly, but Claude models are available on OpenRouter). It was created by Luka Koning and Tjark Van de Merwe.</p>



<h3 class="wp-block-heading" id="the-bottom-line">The bottom line</h3>



<p>The generative AI ecosystem for R is not as robust as Python’s, and that’s unlikely to change. However, in the past year, there’s been a lot of progress in creating tools for key tasks programmers might want to do with LLMs in R. If R is your language of choice and you’re interested in working with large language models either locally or via APIs, it’s worth giving some of these options a try.</p>
</div></div></div></div>
  </div>
  <p>
    Sources:
    <span itemprop="author" itemscope itemtype="https://schema.org/Person">
      <a itemprop="url" href="https://www.infoworld.com/article/3828268/top-llm-integration-tools-for-r-programming.html" rel="external noreferrer nofollow noopener" target="_blank">
        <span itemprop="name">Info World</span>
      </a>
    </span><br>
    Published:
    <span itemprop="datePublished">Mar 11, 2025, 5:00:00 AM EDT</span>
  </p>
</div>
</main>
<footer>
  <div class="links">
    <a href="/terms/">Terms of Service</a> •
    <a href="/privacy/">Privacy Policy</a> •
    <a href="/disclaimer/">Disclaimer</a>
  </div>
  <div class="copy">
    <span class="icons" itemscope itemtype="https://schema.org/Organization">
      <meta itemprop="name" content="Major Digest">
      <meta itemprop="description" content="Reliable and Comprehensive News Sources">
      <meta itemprop="naics" content="513110">
      <link itemprop="url" href="https://majordigest.com/">
      <link itemprop="logo" href="https://majordigest.com/assets/icons/logo-512x512.png">
      <a href="https://x.com/major_digest/" rel="external" target="_blank" itemprop="sameAs" title="Follow us on X (Twitter)" aria-label="Follow us on X (Twitter)"><svg role="img" aria-label="X Logo" xmlns="http://www.w3.org/2000/svg" version="1.1" x="0px" y="0px" width="24px" height="24px" viewBox="0 0 312 312"><path d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg></a>
      <a href="https://www.facebook.com/majordigest" rel="external" target="_blank" itemprop="sameAs" title="Follow us on Facebook" aria-label="Follow us on Facebook"><svg role="img" aria-label="Facebook Logo" xmlns="http://www.w3.org/2000/svg" width="24px" height="24px" x="0px" y="0px" viewBox="0 0 455.73 455.73"><path d="M0,0v455.73h242.704V279.691h-59.33v-71.864h59.33v-60.353c0-43.893,35.582-79.475,79.475-79.475h62.025v64.622h-44.382 c-13.947,0-25.254,11.307-25.254,25.254v49.953h68.521l-9.47,71.864h-59.051V455.73H455.73V0H0z"/></svg></a>
      <a href="https://www.instagram.com/majordigest" rel="external" target="_blank" itemprop="sameAs" title="Follow us on Instagram" aria-label="Follow us on Instagram"><svg role="img" aria-label="Instagram Logo" xmlns="http://www.w3.org/2000/svg" version="1.1" x="0px" y="0px" width="24px" height="24px" viewBox="0 0 169.063 169.063"><path d="M122.406,0H46.654C20.929,0,0,20.93,0,46.655v75.752c0,25.726,20.929,46.655,46.654,46.655h75.752 c25.727,0,46.656-20.93,46.656-46.655V46.655C169.063,20.93,148.133,0,122.406,0z M154.063,122.407 c0,17.455-14.201,31.655-31.656,31.655H46.654C29.2,154.063,15,139.862,15,122.407V46.655C15,29.201,29.2,15,46.654,15h75.752 c17.455,0,31.656,14.201,31.656,31.655V122.407z"></path><path d="M84.531,40.97c-24.021,0-43.563,19.542-43.563,43.563c0,24.02,19.542,43.561,43.563,43.561s43.563-19.541,43.563-43.561 C128.094,60.512,108.552,40.97,84.531,40.97z M84.531,113.093c-15.749,0-28.563-12.812-28.563-28.561 c0-15.75,12.813-28.563,28.563-28.563s28.563,12.813,28.563,28.563C113.094,100.281,100.28,113.093,84.531,113.093z"></path><path d="M129.921,28.251c-2.89,0-5.729,1.17-7.77,3.22c-2.051,2.04-3.23,4.88-3.23,7.78c0,2.891,1.18,5.73,3.23,7.78 c2.04,2.04,4.88,3.22,7.77,3.22c2.9,0,5.73-1.18,7.78-3.22c2.05-2.05,3.22-4.89,3.22-7.78c0-2.9-1.17-5.74-3.22-7.78 C135.661,29.421,132.821,28.251,129.921,28.251z"></path></svg></a>
      <a href="https://www.threads.net/majordigest" rel="external" target="_blank" itemprop="sameAs"  title="Follow us on Threads" aria-label="Follow us on Threads"><svg role="img" aria-label="Threads Logo" width="24px" height="24px" viewBox="0 0 192 192" xmlns="http://www.w3.org/2000/svg"><path d="M141.537 88.9883C140.71 88.5919 139.87 88.2104 139.019 87.8451C137.537 60.5382 122.616 44.905 97.5619 44.745C97.4484 44.7443 97.3355 44.7443 97.222 44.7443C82.2364 44.7443 69.7731 51.1409 62.102 62.7807L75.881 72.2328C81.6116 63.5383 90.6052 61.6848 97.2286 61.6848C97.3051 61.6848 97.3819 61.6848 97.4576 61.6855C105.707 61.7381 111.932 64.1366 115.961 68.814C118.893 72.2193 120.854 76.925 121.825 82.8638C114.511 81.6207 106.601 81.2385 98.145 81.7233C74.3247 83.0954 59.0111 96.9879 60.0396 116.292C60.5615 126.084 65.4397 134.508 73.775 140.011C80.8224 144.663 89.899 146.938 99.3323 146.423C111.79 145.74 121.563 140.987 128.381 132.296C133.559 125.696 136.834 117.143 138.28 106.366C144.217 109.949 148.617 114.664 151.047 120.332C155.179 129.967 155.42 145.8 142.501 158.708C131.182 170.016 117.576 174.908 97.0135 175.059C74.2042 174.89 56.9538 167.575 45.7381 153.317C35.2355 139.966 29.8077 120.682 29.6052 96C29.8077 71.3178 35.2355 52.0336 45.7381 38.6827C56.9538 24.4249 74.2039 17.11 97.0132 16.9405C119.988 17.1113 137.539 24.4614 149.184 38.788C154.894 45.8136 159.199 54.6488 162.037 64.9503L178.184 60.6422C174.744 47.9622 169.331 37.0357 161.965 27.974C147.036 9.60668 125.202 0.195148 97.0695 0H96.9569C68.8816 0.19447 47.2921 9.6418 32.7883 28.0793C19.8819 44.4864 13.2244 67.3157 13.0007 95.9325L13 96L13.0007 96.0675C13.2244 124.684 19.8819 147.514 32.7883 163.921C47.2921 182.358 68.8816 191.806 96.9569 192H97.0695C122.03 191.827 139.624 185.292 154.118 170.811C173.081 151.866 172.51 128.119 166.26 113.541C161.776 103.087 153.227 94.5962 141.537 88.9883ZM98.4405 129.507C88.0005 130.095 77.1544 125.409 76.6196 115.372C76.2232 107.93 81.9158 99.626 99.0812 98.6368C101.047 98.5234 102.976 98.468 104.871 98.468C111.106 98.468 116.939 99.0737 122.242 100.233C120.264 124.935 108.662 128.946 98.4405 129.507Z"></path></svg></a>
      <a href="https://t.me/majordigest" rel="external" target="_blank" itemprop="sameAs" title="Follow us on Telegram" aria-label="Follow us on Telegram"><svg role="img" aria-label="Telegram Logo" width="24px" height="24px" viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke="black" stroke-linecap="round" stroke-linejoin="round" d="M40.83,8.48c1.14,0,2,1,1.54,2.86l-5.58,26.3c-.39,1.87-1.52,2.32-3.08,1.45L20.4,29.26a.4.4,0,0,1,0-.65L35.77,14.73c.7-.62-.15-.92-1.07-.36L15.41,26.54a.46.46,0,0,1-.4.05L6.82,24C5,23.47,5,22.22,7.23,21.33L40,8.69a2.16,2.16,0,0,1,.83-.21Z"/></svg></a>
      <a href="https://www.youtube.com/@MajorDigest" rel="external" target="_blank" itemprop="sameAs" title="Follow us on YouTube" aria-label="Follow us on YouTube"><svg role="img" aria-label="YouTube Logo" height="24px" width="24px" version="1.1" viewBox="0 0 461.001 461.00" xmlns="http://www.w3.org/2000/svg"><path d="M365.257,67.393H95.744C42.866,67.393,0,110.259,0,163.137v134.728 c0,52.878,42.866,95.744,95.744,95.744h269.513c52.878,0,95.744-42.866,95.744-95.744V163.137 C461.001,110.259,418.135,67.393,365.257,67.393z M300.506,237.056l-126.06,60.123c-3.359,1.602-7.239-0.847-7.239-4.568V168.607 c0-3.774,3.982-6.22,7.348-4.514l126.06,63.881C304.363,229.873,304.298,235.248,300.506,237.056z"/></svg></a>
      <a href="https://podcasts.apple.com/us/podcast/major-digest-podcast/id1769748189" rel="external" target="_blank" itemprop="sameAs" title="Follow us on Apple Podcasts" aria-label="Follow us on Apple Podcasts"><svg role="img" aria-label="Apple Podcasts Logo" width="21px" height="21px" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg></a>
      <a href="https://podcasters.spotify.com/pod/show/majordigest/" rel="external" target="_blank" itemprop="sameAs" title="Follow us on Spotify" aria-label="Follow us on Spotify"><svg role="img" aria-label="Spotify Logo" width="28px" height="28px" viewBox="0 -2 30 30" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M13.2 20.84c-0.2 0-0.4-0.080-0.56-0.2-1.84-1.6-5.8-1.12-7.2-0.84-0.44 0.12-0.92-0.2-1-0.64-0.12-0.44 0.2-0.88 0.64-1 0.24-0.040 5.8-1.24 8.64 1.2 0.36 0.32 0.4 0.84 0.080 1.2-0.12 0.16-0.36 0.28-0.6 0.28zM14.2 18.44c-0.16 0-0.32-0.040-0.48-0.16-3.36-2.4-8.48-1.080-8.52-1.080-0.44 0.12-0.92-0.16-1.040-0.6s0.16-0.92 0.6-1.040c0.24-0.080 5.92-1.56 9.96 1.32 0.36 0.28 0.48 0.8 0.2 1.16-0.2 0.28-0.44 0.4-0.72 0.4zM15.24 15.72c-0.16 0-0.32-0.040-0.48-0.16-4.44-2.96-10.040-1.040-10.12-1.040-0.44 0.16-0.88-0.080-1.040-0.52s0.080-0.92 0.52-1.080c0.28-0.080 6.48-2.2 11.6 1.24 0.4 0.24 0.48 0.76 0.24 1.16-0.2 0.24-0.48 0.4-0.72 0.4zM9.6 25.6c-5.28 0-9.6-4.32-9.6-9.6s4.32-9.6 9.6-9.6 9.6 4.32 9.6 9.6-4.32 9.6-9.6 9.6zM9.6 8.080c-4.36 0-7.92 3.56-7.92 7.92s3.56 7.92 7.92 7.92 7.92-3.56 7.92-7.92-3.56-7.92-7.92-7.92z"></path></svg></a>
<!--
      <a href="https://play.google.com/store/apps/details?id=com.majordigest.android" rel="external" target="_blank" itemprop="sameAs" title="Download Android App" aria-label="Download Android App"><svg role="img" aria-label="Google Play Logo" xmlns="http://www.w3.org/2000/svg" width="24px" height="24px" x="0px" y="0px" viewBox="0 0 32 32"><path d="M20.331 14.644l-13.794-13.831 17.55 10.075zM2.938 0c-0.813 0.425-1.356 1.2-1.356 2.206v27.581c0 1.006 0.544 1.781 1.356 2.206l16.038-16zM29.512 14.1l-3.681-2.131-4.106 4.031 4.106 4.031 3.756-2.131c1.125-0.893 1.125-2.906-0.075-3.8zM6.538 31.188l17.55-10.075-3.756-3.756z"/></svg></a>
-->
    </span>
    © 2025&nbsp;<a href="/about/" title="About Major Digest: A Journey from Idea to Publication">Major Digest</a>
    <small> (v.1.1.6)</small>
  </div>
</footer>

<meta itemprop="operatingSystem" content="All">
<meta itemprop="applicationCategory" content="LifestyleApplication">
<meta itemprop="softwareVersion" content="1.1.6">
<div itemprop="offers" itemscope itemtype="https://schema.org/Offer">
  <meta itemprop="price" content="0">
  <meta itemprop="priceCurrency" content="USD">
</div>
<!--
<div itemprop="aggregateRating" itemscope itemtype="https://schema.org/AggregateRating">
  <meta itemprop="ratingValue" content="5">
  <meta itemprop="ratingCount" content="2">
  <link itemprop="sameAs" href="https://play.google.com/store/apps/details?id=com.majordigest.android">
</div>
<div itemprop="potentialAction" itemscope itemtype="https://schema.org/ViewAction">
  <meta itemprop="name" content="Open Major Digest">
  <link itemprop="target" href="https://majordigest.com/">
  <link itemprop="target" href="android-app://com.majordigest.android/http/majordigest.com">
</div>
<link rel="alternate" href="android-app://com.majordigest.android/http/majordigest.com">
-->

<section id="consent-banner" aria-label="Consent Banner" role="dialog">
  By continuing to use this app, you agree to our 
  <a href="/terms/">Terms of Service</a> and <a href="/privacy/">Privacy Policy</a>. 
  You can learn more about how we use cookies by reviewing our 
  <a href="/privacy/">Privacy Policy</a>. 
  <button onclick="this.parentNode.style.display='none'">Close</button>
</section>
<section id="ios-pwa-prompt" aria-label="iOS Installation Prompt" role="dialog">
  To install this app on your device tap
  <svg xmlns="http://www.w3.org/2000/svg" width="16px" viewBox="0 0 20.88 27.25">
    <polyline points="13.13 8 20.38 8 20.38 26.75 0.5 26.75 0.5 8 7.5 8"/>
    <line x1="10.44" y1="17" x2="10.44"/>
    <line x1="10.48" y1="0.38" x2="15.28" y2="5.18"/>
    <line x1="10.44" y1="0.38" x2="5.64" y2="5.18"/>
  </svg>
  and then Add to Home Screen.
  <button onclick="this.parentNode.style.display='none'">Close</button>
</section>
<script src="/assets/script.js?v=1.1.6" async></script>

<script type="application/ld+json">
{
   "@context": "https://schema.org/",
   "@type": "PodcastSeries",
   "image": "https://majordigest.com/assets/icons/logo-512x512.png",
   "url": "https://podcasters.spotify.com/pod/show/majordigest/",
   "name": "Major Digest Podcast - Spotify Podcasts",
   "description": "Your daily dose of tech news, straight to your feed. From AI and software development to emerging innovations, we’ve got you covered. Join our community of tech enthusiasts and industry professionals for daily updates on the future of technology.",
   "webFeed": "https://anchor.fm/s/fb28fbbc/podcast/rss",
   "author": {
     "@type": "Person",
     "name": "Major Digest"
   }
}
</script>
<script type="application/ld+json">
{
   "@context": "https://schema.org/",
   "@type": "PodcastSeries",
   "image": "https://majordigest.com/assets/icons/logo-512x512.png",
   "url": "https://podcasts.apple.com/us/podcast/major-digest-podcast/id1769748189",
   "name": "Major Digest Podcast - Apple Podcasts",
   "description": "Your daily dose of tech news, straight to your feed. From AI and software development to emerging innovations, we’ve got you covered. Join our community of tech enthusiasts and industry professionals for daily updates on the future of technology.",
   "webFeed": "https://anchor.fm/s/fb28fbbc/podcast/rss",
   "author": {
     "@type": "Person",
     "name": "Major Digest"
   }
}
</script>

</body>
</html>
